{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploración — Ejercicio 3 (RAG + Agente Crítico)\n",
        "\n",
        "Este notebook te ayuda a:\n",
        "- Cargar el CSV de laptops\n",
        "- Preparar un subset 200–400\n",
        "- Crear chunks (campo:valor) y construir índice BM25\n",
        "- Probar consultas (retrieve top-k)\n",
        "- Generar respuesta con citas\n",
        "- Pasar respuesta por el agente crítico\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'rank_bm25'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrank_bm25\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BM25Okapi\n\u001b[0;32m      8\u001b[0m pd\u001b[38;5;241m.\u001b[39mset_option(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisplay.max_columns\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m60\u001b[39m)\n\u001b[0;32m      9\u001b[0m pd\u001b[38;5;241m.\u001b[39mset_option(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisplay.width\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m140\u001b[39m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'rank_bm25'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "pd.set_option('display.max_columns', 60)\n",
        "pd.set_option('display.width', 140)\n",
        "\n",
        "# Ajusta si tu archivo está en otra ruta\n",
        "DATA_PATH = r\"data\\\\Laptops_with_technical_specifications.csv\"\n",
        "SUBSET_N = 300\n",
        "RANDOM_SEED = 42\n",
        "TOP_K = 5\n",
        "MAX_ANSWER_WORDS = 120\n",
        "\n",
        "print('Exists?', os.path.exists(DATA_PATH))\n",
        "if os.path.exists(DATA_PATH):\n",
        "    print('File size (MB):', round(os.path.getsize(DATA_PATH)/1024/1024, 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(DATA_PATH)\n",
        "df.columns = [c.strip() for c in df.columns]\n",
        "print('Shape:', df.shape)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Selección de subset reproducible\n",
        "if len(df) > SUBSET_N:\n",
        "    df_sub = df.sample(SUBSET_N, random_state=RANDOM_SEED).reset_index(drop=True)\n",
        "else:\n",
        "    df_sub = df.copy()\n",
        "\n",
        "print('Subset shape:', df_sub.shape)\n",
        "df_sub.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparar columna de ID\n",
        "El enunciado pide citas `[laptop_id:campo]`. Si el CSV no trae `laptop_id`, usamos la primera columna como ID."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'laptop_id' not in df_sub.columns:\n",
        "    first_col = df_sub.columns[0]\n",
        "    df_sub = df_sub.rename(columns={first_col: 'laptop_id'})\n",
        "    print('Renombrado', first_col, '-> laptop_id')\n",
        "\n",
        "df_sub[['laptop_id']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chunking (campo: valor)\n",
        "Creamos chunks por campo con metadatos de cita."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize(text: str):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-z0-9áéíóúüñ\\s]\", \" \", text)\n",
        "    return [t for t in text.split() if len(t) > 1]\n",
        "\n",
        "def make_chunks(row: dict, laptop_id_field: str = 'laptop_id'):\n",
        "    laptop_id = str(row.get(laptop_id_field, ''))\n",
        "    chunks = []\n",
        "    i = 0\n",
        "    for field, value in row.items():\n",
        "        if field == laptop_id_field:\n",
        "            continue\n",
        "        if value is None:\n",
        "            continue\n",
        "        value = str(value).strip()\n",
        "        if value == '' or value.lower() == 'nan':\n",
        "            continue\n",
        "        txt = f\"{field}: {value}\"\n",
        "        chunks.append({\n",
        "            'chunk_id': f\"{laptop_id}_{i}\",\n",
        "            'laptop_id': laptop_id,\n",
        "            'field': field,\n",
        "            'text': txt,\n",
        "            'citations': [(laptop_id, field)]\n",
        "        })\n",
        "        i += 1\n",
        "    return chunks\n",
        "\n",
        "rows = df_sub.to_dict(orient='records')\n",
        "chunks = []\n",
        "for r in rows:\n",
        "    chunks.extend(make_chunks(r))\n",
        "\n",
        "print('Chunks:', len(chunks))\n",
        "pd.DataFrame(chunks[:8])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BM25 Index + búsqueda top-k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corpus_tokens = [tokenize(c['text']) for c in chunks]\n",
        "bm25 = BM25Okapi(corpus_tokens)\n",
        "\n",
        "def search(query: str, top_k: int = 5):\n",
        "    q = tokenize(query)\n",
        "    scores = bm25.get_scores(q)\n",
        "    idx = np.argsort(scores)[::-1][:top_k]\n",
        "    return [(chunks[i], float(scores[i])) for i in idx]\n",
        "\n",
        "query = 'laptop con 16gb ram y ssd'\n",
        "results = search(query, TOP_K)\n",
        "[(r[0]['text'], r[1]) for r in results[:3]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generación de respuesta (MVP) + citas\n",
        "MVP: compone respuesta a partir de los chunks recuperados y agrega citas `[laptop_id:campo]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_answer(query: str, retrieved, max_words: int = 120):\n",
        "    lines = [ch['text'] for ch, _ in retrieved]\n",
        "    answer_core = ' '.join(lines)\n",
        "    answer_core = ' '.join(answer_core.split()[:max_words])\n",
        "\n",
        "    cites = []\n",
        "    seen = set()\n",
        "    for ch, _ in retrieved:\n",
        "        for lid, field in ch['citations']:\n",
        "            key = f\"{lid}:{field}\"\n",
        "            if key not in seen:\n",
        "                cites.append(f\"[{lid}:{field}]\")\n",
        "                seen.add(key)\n",
        "    context_text = '\\n'.join(lines)\n",
        "    return answer_core + ' ' + ' '.join(cites), context_text, cites\n",
        "\n",
        "answer, context, cites = generate_answer(query, results, MAX_ANSWER_WORDS)\n",
        "answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Agente crítico\n",
        "Valida soporte: si detecta afirmaciones sin evidencia (heurística), reescribe/recorta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize(s: str) -> str:\n",
        "    s = s.lower()\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def critic_review(answer: str, context: str, citations: list[str]):\n",
        "    issues = []\n",
        "    if not citations:\n",
        "        issues.append('No citations found.')\n",
        "\n",
        "    ctx = normalize(context)\n",
        "    sentences = [s.strip() for s in re.split(r\"[.!?]\", answer) if s.strip()]\n",
        "\n",
        "    supported = []\n",
        "    for s in sentences:\n",
        "        s_norm = normalize(s)\n",
        "        if len(s_norm.split()) < 4:\n",
        "            supported.append(True)\n",
        "            continue\n",
        "        supported.append(s_norm[:60] in ctx or any(tok in ctx for tok in s_norm.split()[:6]))\n",
        "\n",
        "    if not all(supported):\n",
        "        issues.append('Some statements may be unsupported by retrieved context.')\n",
        "        kept = [sentences[i] for i, ok in enumerate(supported) if ok]\n",
        "        revised = '. '.join(kept)\n",
        "        if revised:\n",
        "            revised_answer = revised + '. ' + ' '.join(citations)\n",
        "        else:\n",
        "            revised_answer = 'No puedo responder con confianza con la evidencia recuperada. ' + ' '.join(citations)\n",
        "        return {'ok': False, 'revised_answer': revised_answer, 'issues': issues}\n",
        "\n",
        "    return {'ok': True, 'revised_answer': answer, 'issues': []}\n",
        "\n",
        "review = critic_review(answer, context, cites)\n",
        "review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Answer (raw):')\n",
        "print(answer)\n",
        "\n",
        "print('\\nAnswer (final):')\n",
        "print(review['revised_answer'])\n",
        "\n",
        "print('\\nIssues:', review['issues'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Siguiente paso\n",
        "1) Curar `data/eval_queries.json` con `relevant_laptop_ids` reales\n",
        "2) Ejecutar evaluación automática (Precision@k, Recall@k, faithfulness, coverage)\n",
        "3) (Opcional) conectar un LLM real manteniendo el contrato de salida y citas\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
